{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35e543c0-e573-43d2-82da-1428abb1aa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import time\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support # Function to evaluate: accuracy, precision, recall, f1-score\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.models import clone_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf2bdfa1-256c-42bc-9375-a3f16fb430b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook last run: 2024-06-24 13:15:50.495922\n"
     ]
    }
   ],
   "source": [
    "print(f\"Notebook last run: {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b45e487-451f-4cca-bdfd-2632f30d29f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_pubmed_data(data_dir):    \n",
    "    train_samples = preprocess_text_with_line_numbers(os.path.join(data_dir, \"train.txt\"))\n",
    "    val_samples = preprocess_text_with_line_numbers(os.path.join(data_dir, \"dev.txt\"))\n",
    "    test_samples = preprocess_text_with_line_numbers(os.path.join(data_dir, \"test.txt\"))\n",
    "\n",
    "    train_df = pd.DataFrame(train_samples)\n",
    "    val_df = pd.DataFrame(val_samples)\n",
    "    test_df = pd.DataFrame(test_samples)\n",
    "\n",
    "    # Convert abstract text lines into lists \n",
    "    train_sentences = train_df[\"text\"].tolist()\n",
    "    val_sentences = val_df[\"text\"].tolist()\n",
    "    test_sentences = test_df[\"text\"].tolist()\n",
    "\n",
    "    return train_df, val_df, test_df, train_sentences, val_sentences, test_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6aefe52c-b082-4043-ada2-1fd0cfdb9df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to read the lines of a document\n",
    "def get_lines(filename):\n",
    "  with open(filename, \"r\") as f:\n",
    "    return f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b77e078-009d-4043-a346-4637fdb2e924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_with_line_numbers(filename):\n",
    "    input_lines = get_lines(filename) # get all lines from filename\n",
    "    abstract_lines = \"\" # create an empty abstract\n",
    "    abstract_samples = [] # create an empty list of abstracts\n",
    "\n",
    "    # Loop through each line in target file\n",
    "    for line in input_lines:\n",
    "        if line.startswith(\"###\"): # check to see if line is an ID line\n",
    "            abstract_id = line\n",
    "            abstract_lines = \"\" # reset abstract string\n",
    "        elif line.isspace(): # check to see if line is a new line\n",
    "            abstract_line_split = abstract_lines.splitlines() # split abstract into separate lines\n",
    "\n",
    "            # Iterate through each line in abstract and count them at the same time\n",
    "            for abstract_line_number, abstract_line in enumerate(abstract_line_split):\n",
    "                line_data = {} # create empty dict to store data from line\n",
    "                target_text_split = abstract_line.split(\"\\t\") # split target label from text\n",
    "                line_data[\"target\"] = target_text_split[0] # get target label\n",
    "                line_data[\"text\"] = target_text_split[1].lower() # get target text and lower it\n",
    "                line_data[\"line_number\"] = abstract_line_number # what number line does the line appear in the abstract?\n",
    "                line_data[\"total_lines\"] = len(abstract_line_split) - 1 # how many total lines are in the abstract? (start from 0)\n",
    "                abstract_samples.append(line_data) # add line data to abstract samples list\n",
    "\n",
    "        else: # if the above conditions aren't fulfilled, the line contains a labelled sentence\n",
    "            abstract_lines += line\n",
    "\n",
    "    return abstract_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1c35992-d571-4ca6-8c1b-e50b657bc8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a TensorBoard callback instand to store log files.\n",
    "def create_tensorboard_callback(dir_name, experiment_name):\n",
    "\n",
    "  log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "      log_dir=log_dir\n",
    "  )\n",
    "  print(f\"Saving TensorBoard log files to: {log_dir}\")\n",
    "  return tensorboard_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2db9c0ed-f625-49d0-b885-45f85303dc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_results(y_true, y_pred):\n",
    "  # Calculate model accuracy\n",
    "  model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "  # Calculate model precision, recall and f1 score using \"weighted average\n",
    "  model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "  model_results = {\"accuracy\": model_accuracy,\n",
    "                  \"precision\": model_precision,\n",
    "                  \"recall\": model_recall,\n",
    "                  \"f1\": model_f1}\n",
    "  return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57d4e661-46e5-47e6-993c-7ddd2e076fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot encode labels\n",
    "def perform_one_hot_encoding(one_hot_encoder, train_df, val_df, test_df, target_column=\"target\"):\n",
    "    train_labels_one_hot = one_hot_encoder.fit_transform(train_df[\"target\"].to_numpy().reshape(-1, 1))\n",
    "    val_labels_one_hot = one_hot_encoder.transform(val_df[\"target\"].to_numpy().reshape(-1, 1))\n",
    "    test_labels_one_hot = one_hot_encoder.transform(test_df[\"target\"].to_numpy().reshape(-1, 1))\n",
    "    return train_labels_one_hot, val_labels_one_hot, test_labels_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31c4ba46-1037-4e0c-8bc5-bf1d2fdbe122",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract labels (\"target\" columns) and encode them into integers \n",
    "def perform_label_encoding(label_encoder, train_df, val_df, test_df, target_column=\"target\"):\n",
    "    train_labels_encoded = label_encoder.fit_transform(train_df[\"target\"].to_numpy())\n",
    "    val_labels_encoded = label_encoder.transform(val_df[\"target\"].to_numpy())\n",
    "    test_labels_encoded =label_encoder.transform(test_df[\"target\"].to_numpy())\n",
    "    return train_labels_encoded, val_labels_encoded, test_labels_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aab3a491-2980-4750-9b87-980c4bf4d0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_chars_in_sentences(sentences):\n",
    "    return [split_chars(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53ba39e0-2fcb-4f21-805d-5ea8da320812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make function to split sentences into characters\n",
    "def split_chars(text):\n",
    "  return \" \".join(list(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db4d64b2-a653-408c-a43a-a98c81e9033c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_datasets(sentences, labels_one_hot, batch_size=32):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((sentences, labels_one_hot))\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d3d2186-707f-406e-aa36-afd18629c1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_char_token_datasets(sentences, chars, labels_one_hot, batch_size=32, prefetch_buffer=tf.data.AUTOTUNE):\n",
    "    data = tf.data.Dataset.from_tensor_slices((sentences, chars))\n",
    "    labels = tf.data.Dataset.from_tensor_slices(labels_one_hot)\n",
    "    dataset = tf.data.Dataset.zip((data, labels))\n",
    "    dataset = dataset.batch(batch_size).prefetch(prefetch_buffer)\n",
    "  \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5dafd6f8-be0e-403b-9bb8-1c0755c4ba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_position_char_token_datasets(line_numbers_one_hot, total_lines_one_hot, sentences, chars, labels_one_hot, batch_size=32, prefetch_buffer=tf.data.AUTOTUNE):\n",
    "    data = tf.data.Dataset.from_tensor_slices((line_numbers_one_hot, total_lines_one_hot, sentences, chars))\n",
    "    labels = tf.data.Dataset.from_tensor_slices(labels_one_hot)\n",
    "    dataset = tf.data.Dataset.zip((data, labels))\n",
    "    dataset = dataset.batch(batch_size).prefetch(prefetch_buffer)\n",
    "  \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a72b35ab-0696-408f-b267-6a16a43be52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # 1. Token inputs\n",
    "    token_inputs = layers.Input(shape=[], dtype=\"string\", name=\"token_inputs\")\n",
    "    token_embeddings = use_layer(token_inputs)\n",
    "    token_outputs = layers.Dense(128, activation=\"relu\")(token_embeddings)\n",
    "    token_model = tf.keras.Model(inputs=token_inputs,\n",
    "                                 outputs=token_outputs)\n",
    "    \n",
    "    # 2. Char inputs\n",
    "    char_inputs = layers.Input(shape=(1,), dtype=\"string\", name=\"char_inputs\")\n",
    "    char_vectors = char_vectorizer(char_inputs)\n",
    "    char_embeddings = char_embed(char_vectors)\n",
    "    char_bi_lstm = layers.Bidirectional(layers.LSTM(32))(char_embeddings)\n",
    "    char_model = tf.keras.Model(inputs=char_inputs,\n",
    "                                outputs=char_bi_lstm)\n",
    "    \n",
    "    # 3. Line numbers inputs\n",
    "    line_number_inputs = layers.Input(shape=(15,), dtype=tf.int32, name=\"line_number_input\")\n",
    "    x = layers.Dense(32, activation=\"relu\")(line_number_inputs)\n",
    "    line_number_model = tf.keras.Model(inputs=line_number_inputs,\n",
    "                                       outputs=x)\n",
    "    \n",
    "    # 4. Total lines inputs\n",
    "    total_lines_inputs = layers.Input(shape=(20,), dtype=tf.int32, name=\"total_lines_input\")\n",
    "    y = layers.Dense(32, activation=\"relu\")(total_lines_inputs)\n",
    "    total_line_model = tf.keras.Model(inputs=total_lines_inputs,\n",
    "                                      outputs=y)\n",
    "    \n",
    "    # 5. Combine token and char embeddings into a hybrid embedding\n",
    "    combined_embeddings = layers.Concatenate(name=\"token_char_hybrid_embedding\")([token_model.output, \n",
    "                                                                                  char_model.output])\n",
    "    z = layers.Dense(256, activation=\"relu\")(combined_embeddings)\n",
    "    z = layers.Dropout(0.5)(z)\n",
    "    \n",
    "    # 6. Combine positional embeddings with combined token and char embeddings into a tribrid embedding\n",
    "    z = layers.Concatenate(name=\"token_char_positional_embedding\")([line_number_model.output,\n",
    "                                                                    total_line_model.output,\n",
    "                                                                    z])\n",
    "    \n",
    "    # 7. Create output layer\n",
    "    output_layer = layers.Dense(5, activation=\"softmax\", name=\"output_layer\")(z)\n",
    "    \n",
    "    # 8. Put together model\n",
    "    model = tf.keras.Model(inputs=[line_number_model.input,\n",
    "                                     total_line_model.input,\n",
    "                                     token_model.input, \n",
    "                                     char_model.input],\n",
    "                                     outputs=output_layer)\n",
    "        \n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
